{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe for wind verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                      # Data\n",
    "import pandas as pd                     # Data \n",
    "import geopandas as gpd                 # Data\n",
    "import xarray as xr                     # Data\n",
    "import atlite                           # Model\n",
    "import matplotlib.pyplot as plt         # Plot\n",
    "from matplotlib.lines import Line2D     # Plot\n",
    "from tqdm import tqdm                   # Visualise progression in loop\n",
    "from yaml import safe_load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EirGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations of farms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file located in /Data_Final folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capacity = pd.read_csv('../Data/capacity_wind_9224_eir.csv',\n",
    "                               index_col = 2,\n",
    "                               parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns to be correctly read by Atlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capacity = df_capacity.dropna().rename(columns={'Capacity (MW)':'capacity', 'Latitude':'y', 'Longitude':'x'})\n",
    "df_capacity = df_capacity[df_capacity.index < '2024'] # Make sure to remove all data before 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cutout for Ireland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weather files downloaded on Copernicus website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boris/anaconda3/lib/python3.11/site-packages/gribapi/__init__.py:23: UserWarning: ecCodes 2.31.0 or higher is recommended. You are running version 2.24.2\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/boris/Documents/GitHub/RES_droughts/Data/ERA5/ERA5_100m_u_v_hourly_2009_2018.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/boris/Documents/GitHub/RES_droughts/Data/ERA5/ERA5_100m_u_v_hourly_2009_2018.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '12b39020-6c39-4589-984b-85c513e714f4']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_uv100_to2018 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/ERA5/ERA5_100m_u_v_hourly_2009_2018.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m ds_uv100_2019_2023 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/ERA5/ERA5_100m_u_v_hourly_2019_2023.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m ds_roughness_2018_2023 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/ERA5/ERA5_forecast_surface_roughness_geopotential_hourly_2018_2023.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:566\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    555\u001b[0m     decode_cf,\n\u001b[1;32m    556\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 566\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    567\u001b[0m     filename_or_obj,\n\u001b[1;32m    568\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    572\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    573\u001b[0m     backend_ds,\n\u001b[1;32m    574\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m )\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:590\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    571\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    588\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    589\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 590\u001b[0m     store \u001b[38;5;241m=\u001b[39m NetCDF4DataStore\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    591\u001b[0m         filename_or_obj,\n\u001b[1;32m    592\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m    594\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[1;32m    595\u001b[0m         clobber\u001b[38;5;241m=\u001b[39mclobber,\n\u001b[1;32m    596\u001b[0m         diskless\u001b[38;5;241m=\u001b[39mdiskless,\n\u001b[1;32m    597\u001b[0m         persist\u001b[38;5;241m=\u001b[39mpersist,\n\u001b[1;32m    598\u001b[0m         lock\u001b[38;5;241m=\u001b[39mlock,\n\u001b[1;32m    599\u001b[0m         autoclose\u001b[38;5;241m=\u001b[39mautoclose,\n\u001b[1;32m    600\u001b[0m     )\n\u001b[1;32m    602\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    385\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    386\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    388\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    389\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(manager, group\u001b[38;5;241m=\u001b[39mgroup, mode\u001b[38;5;241m=\u001b[39mmode, lock\u001b[38;5;241m=\u001b[39mlock, autoclose\u001b[38;5;241m=\u001b[39mautoclose)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:338\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:400\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:394\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    395\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2469\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2028\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/boris/Documents/GitHub/RES_droughts/Data/ERA5/ERA5_100m_u_v_hourly_2009_2018.nc'"
     ]
    }
   ],
   "source": [
    "ds_uv100_to2018 = xr.open_dataset('../Data/ERA5/ERA5_100m_u_v_hourly_2009_2018.nc')\n",
    "ds_uv100_2019_2023 = xr.open_dataset('../Data/ERA5/ERA5_100m_u_v_hourly_2019_2023.nc')\n",
    "ds_roughness_2018_2023 = xr.open_dataset('../Data/ERA5/ERA5_forecast_surface_roughness_geopotential_hourly_2018_2023.nc')\n",
    "ds_roughness_to2017 = xr.open_dataset('../Data/ERA5/ERA5_forecast_surface_roughness_geopotential_hourly_2012_2017.nc')\n",
    "ds_uv100_to2018 = ds_uv100_to2018.sel(time=slice(\"2014-01-01\", \"2018-12-31\"))\n",
    "ds_roughness_to2018 = ds_roughness_to2017.sel(time=slice(\"2014-01-01\", \"2017-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables need to be formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_uv100_2019_2023 = ds_uv100_2019_2023.reduce(np.nansum, dim='expver',keep_attrs=True)\n",
    "ds_roughness_2018_2023 = ds_roughness_2018_2023.reduce(np.nansum, dim='expver',keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset does not contain the dimensions: ['expver']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_uv100_2019_2023 \u001b[38;5;241m=\u001b[39m ds_uv100_2019_2023\u001b[38;5;241m.\u001b[39mreduce(np\u001b[38;5;241m.\u001b[39mnansum, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpver\u001b[39m\u001b[38;5;124m'\u001b[39m,keep_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m ds_roughness_2018_2023 \u001b[38;5;241m=\u001b[39m ds_roughness_2018_2023\u001b[38;5;241m.\u001b[39mreduce(np\u001b[38;5;241m.\u001b[39mnansum, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpver\u001b[39m\u001b[38;5;124m'\u001b[39m,keep_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/dataset.py:5941\u001b[0m, in \u001b[0;36mDataset.reduce\u001b[0;34m(self, func, dim, keep_attrs, keepdims, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   5939\u001b[0m missing_dimensions \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[1;32m   5940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_dimensions:\n\u001b[0;32m-> 5941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5942\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset does not contain the dimensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_dimensions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5943\u001b[0m     )\n\u001b[1;32m   5945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5946\u001b[0m     keep_attrs \u001b[38;5;241m=\u001b[39m _get_keep_attrs(default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset does not contain the dimensions: ['expver']"
     ]
    }
   ],
   "source": [
    "ds_uv100_2019_2023 = ds_uv100_2019_2023.reduce(np.nansum, dim='expver',keep_attrs=True)\n",
    "ds_roughness_2018_2023 = ds_roughness_2018_2023.reduce(np.nansum, dim='expver',keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_uv100 = xr.concat([ds_uv100_to2018.rename({'lat':'latitude', 'lon':'longitude'}), ds_uv100_2019_2023], dim='time')\n",
    "ds_roughness = xr.concat([ds_roughness_to2017, ds_roughness_2018_2023], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge([ds_uv100, ds_roughness])\n",
    "ds = ds.sel(time=slice(\"2014-01-01\", \"2023-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the climate variables in Atlite\n",
    "\n",
    "This can only be done via the function get_cutout_from_era5_data which is not originally in the Atlite scripts but has been added by the authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_ie = atlite.cutout.get_cutout_from_era5_data('path', ds, ['wind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to create a time dependent layout of the capacity from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dependent_capacity_distribution(cutout_ie: atlite.Cutout, df_capacity: pd.DataFrame):\n",
    "    capacity_layout = cutout_ie.data['roughness'].copy()\n",
    "    capacity_layout.name = 'Capacity'\n",
    "    capacity_layout[:,:,:] = 0.\n",
    "\n",
    "    # Iterate over all capacity installations\n",
    "    for idx, row in tqdm(df_capacity.reset_index().iterrows(), total= df_capacity.shape[0]):\n",
    "        cap = row['capacity'] # Capacity values\n",
    "        df_capacity_i = pd.DataFrame([row])\n",
    "        layout = cutout_ie.layout_from_capacity_list(df_capacity_i, col=\"capacity\")\n",
    "\n",
    "        capacity_layout[capacity_layout['time'] >= row['Connection date']] += layout\n",
    "\n",
    "    return capacity_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get atlite generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf_series_atlite(cutout_ie: atlite.Cutout, capacity_layout, power_curve):\n",
    "    if isinstance(power_curve, dict):\n",
    "        # Dict contains wind_speed, power, power_max, hub_height\n",
    "        wind_cf = atlite.convert.convert_wind(cutout_ie.data, power_curve)\n",
    "    elif isinstance(power_curve, str):\n",
    "        # Str name of turbine which atlite knows\n",
    "        wind_cf = atlite.convert.convert_wind(cutout_ie.data, atlite.resource.get_windturbineconfig(power_curve))\n",
    "\n",
    "    if isinstance(capacity_layout, xr.DataArray):\n",
    "        # What supposed to happen\n",
    "        return wind_cf.weighted(capacity_layout).mean(('x','y'))\n",
    "    elif isinstance(capacity_layout, pd.DataFrame):\n",
    "        # Take this path if layout is not processed by us\n",
    "        return wind_cf.weighted(get_time_dependent_capacity_distribution(cutout_ie, capacity_layout).mean(('x','y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turbines\n",
    "\n",
    "On an analysis of multiple turbines and their smoothing, we identify the Enercon.E112.4500 with 0.30w from renewables.ninja as the turbine with the best representation of EirGrid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Enercon Enercon.E112.4500 power curve and the smoothed version as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turbines_ninja_nosmoothing = pd.read_csv('../Data/renewables_ninja/Wind Turbine Power Curves ~ 5 (0.01ms with 0.00 w smoother).csv')\n",
    "turbine_ninja_nosmoothing = {'hub_height':100, 'P':1., 'V':df_turbines_ninja_nosmoothing['speed'].values, 'POW':df_turbines_ninja_nosmoothing['Enercon.E112.4500'].values}\n",
    "\n",
    "df_turbines_ninja_030smoothing = pd.read_csv('../Data/renewables_ninja/Wind Turbine Power Curves ~ 5 (0.01ms with 0.30 w smoother).csv')\n",
    "turbine_ninja_030smoothing = {'hub_height':100, 'P':1., 'V':df_turbines_ninja_030smoothing['data$speed'].values, 'POW':df_turbines_ninja_030smoothing['Enercon.E112.4500'].values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For C3S-Energy the wind turbine power curve is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_turbines_vestas = '../Data/power_curve_Vestas_V136_3450_c3se.yaml'\n",
    "with open(path_turbines_vestas, 'r') as f:\n",
    "    df_turbines_vestas = pd.json_normalize(safe_load(f))\n",
    "    turbine_vestas = {'hub_height':100, 'P':1, 'V':np.array(df_turbines_vestas['V'].values[0]), 'POW':np.array(df_turbines_vestas['NPOW'].values[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capacity Factor\n",
    "\n",
    "Now we calculate the wind capacity factor series for both the smoothed and non-smoothed curves. We will compare the two to EirGrid before comparing the different methods to visualise the effective difference this makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [01:00<00:00,  6.34it/s]\n"
     ]
    }
   ],
   "source": [
    "full_time_layout = get_time_dependent_capacity_distribution(cutout_ie=cutout_ie, df_capacity=df_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_atlite = get_cf_series_atlite(cutout_ie=cutout_ie, capacity_layout=full_time_layout, power_curve=turbine_ninja_030smoothing)\n",
    "cf_atlite_nosmooth = get_cf_series_atlite(cutout_ie=cutout_ie, capacity_layout=full_time_layout, power_curve=turbine_ninja_nosmoothing)\n",
    "cf_atlite_vestas = get_cf_series_atlite(cutout_ie=cutout_ie, capacity_layout=full_time_layout, power_curve=turbine_vestas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3S-E Gridded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CF data from C3S-Energy from 2014 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_gridded = xr.open_dataarray('../Data/C3S-E/c3se_onshorewind_capacityfactor_20140101_20231231_gridded_ireland.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf_series_c3se_gridded(da_gridded: xr.DataArray, df_capacity: pd.DataFrame, only_cf: bool = True):\n",
    "    lats = da_gridded['latitude'].values\n",
    "    lons = da_gridded['longitude'].values\n",
    "\n",
    "    error_farms_idx = []\n",
    "\n",
    "    summed_time_series = np.zeros(da_gridded['time'].shape)\n",
    "    total_capacity_time_series  = np.zeros(da_gridded['time'].shape)\n",
    "\n",
    "    # Iterate over all capacity installations\n",
    "    for idx, row in df_capacity.reset_index().iterrows():\n",
    "        x = row['x']\n",
    "        y = row['y']\n",
    "        cap = row['capacity']\n",
    "\n",
    "    # Find the nearest lat/lon point\n",
    "        dif_min_lon = np.argmin(abs(lons-x))\n",
    "        dif_min_lat = np.argmin(abs(lats-y))\n",
    "\n",
    "        time_series = da_gridded[:, dif_min_lat, dif_min_lon]\n",
    "\n",
    "        pairs_list = [[0,0], [0,1], [1,0], [1,1], [2,0], [0,2], [2,1], [1,2]]\n",
    "        if np.isnan(time_series).sum() > 2:\n",
    "            for pair in pairs_list:\n",
    "                dif_min_lon = np.argsort(abs(lons-x))[pair[0]]\n",
    "                dif_min_lat = np.argsort(abs(lats-y))[pair[1]]\n",
    "                time_series = da_gridded[:, dif_min_lat, dif_min_lon]\n",
    "                if np.isnan(time_series).sum() <= 2:\n",
    "                    break\n",
    "\n",
    "        capacity_time_series = time_series.copy()\n",
    "        capacity_time_series[:] = cap\n",
    "\n",
    "        time_series[time_series['time']<row['Connection date']] = 0.\n",
    "        capacity_time_series[capacity_time_series['time']<row['Connection date']] = 0.\n",
    "\n",
    "    # Add the mean CF time series to the total multiplied by the capacity (weight)\n",
    "        if np.isnan(time_series).sum() <= 2:\n",
    "            summed_time_series += cap*time_series\n",
    "            total_capacity_time_series += capacity_time_series\n",
    "        else:\n",
    "            print(idx, row)\n",
    "\n",
    "    # Divide the total time series by the total IC to go back to CF\n",
    "    cf_time_series = summed_time_series/total_capacity_time_series\n",
    "    if only_cf:\n",
    "        return cf_time_series\n",
    "    return cf_time_series, summed_time_series, total_capacity_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_c3se_gridded = get_cf_series_c3se_gridded(da_gridded=da_gridded, df_capacity=df_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3S-E National"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV file containing Capacity Factor from C3S-E National"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nat = pd.read_csv('../Data/C3S-E/c3se_onshorewind_capacityfactor_national.csv',\n",
    "                         skiprows = 52,\n",
    "                         usecols = [0,18], # Retrieve IE data\n",
    "                         index_col = 0,\n",
    "                         parse_dates = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Time of verification (2014 - 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nat = df_nat[(\"2014\" <= df_nat.index) & (df_nat.index < \"2024\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV file containing Capacity Factor from C3S-E Subnational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv('../Data/C3S-E/c3se_onshorewind_capacityfactor_subnational.csv',\n",
    "                     skiprows = 52,\n",
    "                     usecols = [0,350], # Data for NI\n",
    "                     index_col = 0,\n",
    "                     parse_dates = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Time of verification (2014 - 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub[(\"2014\" <= df_sub.index) & (df_sub.index < \"2024\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:46<00:00,  6.78it/s]\n",
      "100%|██████████| 66/66 [00:09<00:00,  6.60it/s]\n"
     ]
    }
   ],
   "source": [
    "capacity_series_ie = get_time_dependent_capacity_distribution(cutout_ie=cutout_ie, df_capacity=df_capacity[df_capacity['ROI/NI']=='ROI'])\n",
    "capacity_series_ni = get_time_dependent_capacity_distribution(cutout_ie=cutout_ie, df_capacity=df_capacity[df_capacity['ROI/NI']=='NI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the results into one time series for each distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_series_ie = capacity_series_ie.sum(dim=['x','y'])\n",
    "capacity_series_ni = capacity_series_ni.sum(dim=['x','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "The C3S-E National data is missing data for the 31st of December of 2019. We add it here and make it NaN in order to facilitate further treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nat.loc[pd.to_datetime('2019-12-31T22:00'), 'IE'] = np.nan\n",
    "df_nat.loc[pd.to_datetime('2019-12-31T23:00'), 'IE'] = np.nan\n",
    "df_sub.loc[pd.to_datetime('2019-12-31T22:00'), 'UKN0'] = np.nan\n",
    "df_sub.loc[pd.to_datetime('2019-12-31T23:00'), 'UKN0'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resort the data to avoid future issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nat = df_nat.sort_index()\n",
    "df_sub = df_sub.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we average the two times series with capacities as weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_c3se_national = (df_nat['IE']*capacity_series_ie + df_sub['UKN0']*capacity_series_ni) / (capacity_series_ie + capacity_series_ni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EirGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eirgrid_qtr = pd.read_csv('../Data_Final/EirGrid/eirgrid_qtr_15min_wind_generation_availability_20140101_20231231.csv',\n",
    "                          index_col = 0,\n",
    "                          parse_dates = True)\n",
    "eirgrid_qtr = eirgrid_qtr[(\"2014\" <= eirgrid_qtr.index) & (eirgrid_qtr.index < \"2024\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_eirgrid = eirgrid_qtr['Availability'].resample('1h').mean() / full_time_layout.sum(dim=['x','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data as csv file to be used again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'time':cf_eirgrid.index,\n",
    "                    'EirGrid':cf_eirgrid.values,\n",
    "                    'Atlite': cf_atlite.values,\n",
    "                    'Atlite': cf_atlite_vestas.values,\n",
    "                    'C3S Gridded': cf_c3se_gridded.values,\n",
    "                    'C3S National': cf_c3se_national.values}\n",
    "                )\n",
    "df = df.set_index('time')\n",
    "df.to_csv('../Data/verification_cf_wind_1423.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
